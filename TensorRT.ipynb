{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch model --> TensorRT examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load and launch a pretrained model using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define preprocessing step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    # Transform for image\n",
    "    transforms = Compose([\n",
    "        Resize(224, 224, interpolation=cv2.INTER_NEAREST),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # Read image with cv2\n",
    "    input_img = cv2.imread(img_path)\n",
    "    \n",
    "    # Transform image\n",
    "    input_data = transforms(image=input_img)['image']\n",
    "    \n",
    "    # Convert to batch 1 image\n",
    "    batch_data = torch.unsqueeze(input_data, 0)\n",
    "    return batch_data\n",
    "\n",
    "input = preprocess_image(\"resources/turkish_coffee.jpg\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference and Postprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference with model\n",
    "model.eval()\n",
    "model.cuda()\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: cup , confidence: 94.98472595214844 %, index: 968\n",
      "Class: espresso , confidence: 3.7401280403137207 %, index: 967\n",
      "Class: coffee_mug , confidence: 0.6143011450767517 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "def postprocess(output_data):\n",
    "    # Get class name\n",
    "    with open('resources/imagenet_classes.txt') as f:\n",
    "        classes = [line.split(',')[1].strip() for line in f.readlines()]\n",
    "    \n",
    "    # Calculate score  \n",
    "    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
    "    \n",
    "    # Find top predicted classes\n",
    "    _, indices = torch.sort(output_data, descending=True)\n",
    "    i = 0\n",
    "    \n",
    "    while confidences[indices[0][i]] > 0.5:\n",
    "        class_idx = indices[0][i]\n",
    "        \n",
    "        print(\n",
    "            \"Class:\",\n",
    "            classes[class_idx],\n",
    "            \", confidence:\",\n",
    "            confidences[class_idx].item(),\n",
    "            \"%, index:\",\n",
    "            class_idx.item()\n",
    "        )\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "postprocess(output)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert the pytorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONNX_FILE_PATH = 'results/resnet50.onnx'\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"input\":{\n",
    "        0: \"batch_size\"\n",
    "    }, \n",
    "    \"output\":{\n",
    "        0: \"batch_size\"\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input,\n",
    "    ONNX_FILE_PATH,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    export_params=True,\n",
    "    dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(ONNX_FILE_PATH)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use netron to visualize onnx model\n",
    "# !netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Initialize model in TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build TensorRT serialized engine and save it\n",
    "def build_serialized_engine(logger, onnx_file_path, save_path, min_batch=1, opt_batch=8, max_batch=32, workspace_size=1):\n",
    "    start = time.time()\n",
    "    explicit_batch_flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    \n",
    "    # Initialize TensorRT engine and parse ONNX model\n",
    "    with trt.Builder(logger) as builder, \\\n",
    "        builder.create_network(explicit_batch_flag) as network, \\\n",
    "        builder.create_builder_config() as config:\n",
    "        \n",
    "        # Parse ONNX to network\n",
    "        parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            print('Begin parsing ONNX file')\n",
    "            parser.parse(model.read())\n",
    "        print('Completed parsing ONNX model')\n",
    "\n",
    "        # Config builder\n",
    "        config = builder.create_builder_config()\n",
    "        # allow TensorRT to use up to 1GB of GPU memory for tactic selection\n",
    "        if workspace_size != 0:\n",
    "            config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_size * (1024 * 1024))\n",
    "        # use FP16 mode if possible\n",
    "        if builder.platform_has_fast_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        # Add optimization profile for 'input' (required when using dynamic batch size)\n",
    "        profile = builder.create_optimization_profile()\n",
    "        profile.set_shape(\"input\", (min_batch, 3, 224, 224), (opt_batch, 3, 224, 224), (max_batch, 3, 224, 224))\n",
    "        config.add_optimization_profile(profile)\n",
    "\n",
    "        # generate TensorRT engine optimized for the target platform\n",
    "        print('Building an engine...')\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        # engine = runtime.deserialize_cuda_engine(plan)\n",
    "        # context = engine.create_execution_context()\n",
    "        print(\"Completed creating Engine after {:.2f}s\".format(time.time() - start))\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(serialized_engine)\n",
    " \n",
    "        print('Write serialize engine to {}!'.format(save_path))\n",
    "    \n",
    "\n",
    "# Deserialized engine\n",
    "def build_deserialized_engine(logger, engine_path):\n",
    "    \n",
    "    # Load saved engine\n",
    "    with open (engine_path, 'rb') as f:\n",
    "        serialized_engine = f.read()\n",
    "    \n",
    "    # Deserialize engine\n",
    "    with trt.Runtime(logger) as runtime:\n",
    "        engine = runtime.deserialize_cuda_engine(serialized_engine)\n",
    "    \n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin parsing ONNX file[02/24/2023-13:14:58] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "\n",
      "Completed parsing ONNX model\n",
      "Building an engine...\n",
      "[02/24/2023-13:14:58] [TRT] [W] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.5.0\n",
      "[02/24/2023-13:16:00] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:00] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:00] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:00] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:00] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:01] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:03] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:03] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:03] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:03] [TRT] [W] Try increasing the workspace size to 4194304 bytes to get better performance.\n",
      "[02/24/2023-13:16:03] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[02/24/2023-13:16:03] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[02/24/2023-13:16:03] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[02/24/2023-13:16:03] [TRT] [W] - 80 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[02/24/2023-13:16:03] [TRT] [W] - 37 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "Completed creating Engine after 65.18s\n",
      "Write serialize engine to results/resnet50.engine!\n"
     ]
    }
   ],
   "source": [
    "# Logger to capture errors, warnings and other infomation\n",
    "TRT_LOGGER = trt.Logger()\n",
    "\n",
    "# Build engine\n",
    "SERIALIZED_ENGINE_PATH = 'results/resnet50.engine'\n",
    "\n",
    "# Initialize TensorRT engine and parse ONNX model\n",
    "# batch_size = 1\n",
    "workspace_size = 2 # GB\n",
    "\n",
    "build_serialized_engine(\n",
    "    TRT_LOGGER,\n",
    "    ONNX_FILE_PATH,\n",
    "    SERIALIZED_ENGINE_PATH,\n",
    "    workspace_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_context(engine, input_shape, stream_handle):\n",
    "    context = engine.create_execution_context()\n",
    "    context.set_optimization_profile_async(0, stream_handle)\n",
    "    context.set_input_shape('input', trt.Dims4(list(input_shape)))\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Load and deserialize engine\n",
    "    print('Loading and deserializing engine')\n",
    "    engine = build_deserialized_engine(TRT_LOGGER, SERIALIZED_ENGINE_PATH)\n",
    "    \n",
    "    # Inference batch size\n",
    "    batch_size = 4\n",
    "    sample_image = preprocess_image(\"resources/turkish_coffee.jpg\").numpy()\n",
    "    batch_images = np.concatenate([sample_image] * batch_size, axis=0)\n",
    "    \n",
    "    # Get sizes of input and output and allocate memory required for input and output data\n",
    "    for idx, _tensor in enumerate(engine): # inputs and outputs\n",
    "        name = engine.get_tensor_name(idx)\n",
    "        \n",
    "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT: # in case it is input\n",
    "            input_shape = engine.get_tensor_shape(_tensor)\n",
    "            input_shape[0] = batch_size\n",
    "            \n",
    "            input_size = trt.volume(input_shape) * np.dtype(np.float32).itemsize # in bytes\n",
    "            device_input = cuda.mem_alloc(input_size)\n",
    "        else: # output\n",
    "            output_shape = engine.get_tensor_shape(_tensor)\n",
    "            output_shape[0] = batch_size\n",
    "            \n",
    "            # Create page-locked memory buffer (i.e. won't be swapped to disk)\n",
    "            host_output = cuda.pagelocked_empty(trt.volume(output_shape), dtype=np.float32)\n",
    "            device_output = cuda.mem_alloc(host_output.nbytes)\n",
    "            \n",
    "    # Create a stream in which to copy inputs/outputs and run inference\n",
    "    stream = cuda.Stream()\n",
    "    \n",
    "    print('Inferencing')\n",
    "    \n",
    "    # Preprocess input data\n",
    "    host_input = np.array(batch_images, dtype=np.float32, order='C')\n",
    "    cuda.memcpy_htod_async(device_input, host_input, stream)\n",
    "    \n",
    "    # Create context\n",
    "    context = create_context(engine, host_input.shape, stream.handle)\n",
    "    \n",
    "    # Run inference\n",
    "    context.execute_async_v2(bindings=[int(device_input), int(device_output)], stream_handle=stream.handle)\n",
    "    cuda.memcpy_dtoh_async(host_output, device_output, stream)\n",
    "    stream.synchronize()\n",
    "    \n",
    "    # Post processing (return value in host_output is one dimension array)\n",
    "    tensor_output = torch.Tensor(host_output)\n",
    "    output_data = tensor_output.reshape(batch_size, int(tensor_output.shape[0] / batch_size))\n",
    "    for i, output in enumerate(output_data):\n",
    "        print('=== Image {} ==='.format(i))\n",
    "        postprocess(output.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and deserializing engine\n",
      "Inferencing\n",
      "[02/24/2023-13:31:55] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "=== Image 0 ===\n",
      "Class: cup , confidence: 94.96162414550781 %, index: 968\n",
      "Class: espresso , confidence: 3.7400457859039307 %, index: 967\n",
      "Class: coffee_mug , confidence: 0.6201604008674622 %, index: 504\n",
      "=== Image 1 ===\n",
      "Class: cup , confidence: 94.96162414550781 %, index: 968\n",
      "Class: espresso , confidence: 3.7400457859039307 %, index: 967\n",
      "Class: coffee_mug , confidence: 0.6201604008674622 %, index: 504\n",
      "=== Image 2 ===\n",
      "Class: cup , confidence: 94.96162414550781 %, index: 968\n",
      "Class: espresso , confidence: 3.7400457859039307 %, index: 967\n",
      "Class: coffee_mug , confidence: 0.6201604008674622 %, index: 504\n",
      "=== Image 3 ===\n",
      "Class: cup , confidence: 94.96162414550781 %, index: 968\n",
      "Class: espresso , confidence: 3.7400457859039307 %, index: 967\n",
      "Class: coffee_mug , confidence: 0.6201604008674622 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_MODULE_LOADING']='LAZY'\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*(1, 2, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "67bdd53692923fa214cebb8ea68c1862c18ab74c5be02efebed96b777e6a4c2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
